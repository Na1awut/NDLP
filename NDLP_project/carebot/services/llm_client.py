"""
LLM Client â€” Groq API for CareBot

Uses Groq SDK for fast LLM inference.
Models:
  - llama-3.1-8b-instant: à¸ªà¸à¸±à¸”à¸­à¸²à¸£à¸¡à¸“à¹Œ (fast)
  - llama-3.3-70b-versatile: à¸ªà¸£à¹‰à¸²à¸‡à¸„à¸³à¸•à¸­à¸š (smart)
"""
import os
from typing import Optional
from groq import AsyncGroq


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Configuration
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FAST_MODEL = "llama-3.1-8b-instant"
SMART_MODEL = "llama-3.3-70b-versatile"


class LLMClient:
    """
    Dual-model Groq LLM client.
    Fast model for emotion extraction, smart model for response generation.
    """

    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv("GROQ_API_KEY", "")
        self.client = AsyncGroq(api_key=self.api_key)

    async def extract_emotion(self, text: str, system_prompt: str) -> Optional[str]:
        """
        à¸ªà¸à¸±à¸”à¸­à¸²à¸£à¸¡à¸“à¹Œà¸ˆà¸²à¸à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡ (à¹ƒà¸Šà¹‰ fast model)
        Returns raw JSON string from LLM
        """
        try:
            response = await self.client.chat.completions.create(
                model=FAST_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": text},
                ],
                temperature=0.1,
                max_tokens=300,
                response_format={"type": "json_object"},
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"[LLMClient] Emotion extraction failed: {e}")
            return None

    async def generate_response(
        self,
        user_message: str,
        system_prompt: str,
        evc_context: str = "",
        conversation_history: Optional[list[dict]] = None,
    ) -> str:
        """
        à¸ªà¸£à¹‰à¸²à¸‡à¸„à¸³à¸•à¸­à¸š (à¹ƒà¸Šà¹‰ smart model)
        """
        messages = [{"role": "system", "content": system_prompt}]

        if evc_context:
            messages.append({
                "role": "system",
                "content": f"[EVC Context]\n{evc_context}"
            })

        if conversation_history:
            for msg in conversation_history[-10:]:
                messages.append(msg)

        messages.append({"role": "user", "content": user_message})

        try:
            response = await self.client.chat.completions.create(
                model=SMART_MODEL,
                messages=messages,
                temperature=0.7,
                max_tokens=500,
            )
            return response.choices[0].message.content or "à¸‚à¸­à¹‚à¸—à¸©à¸™à¸° à¸•à¸­à¸™à¸™à¸µà¹‰à¸•à¸­à¸šà¹„à¸¡à¹ˆà¹„à¸”à¹‰"
        except Exception as e:
            print(f"[LLMClient] Response generation failed: {e}")
            return "à¸‚à¸­à¹‚à¸—à¸©à¸™à¸° à¸£à¸°à¸šà¸šà¸¡à¸µà¸›à¸±à¸à¸«à¸²à¸­à¸¢à¸¹à¹ˆ à¸¥à¸­à¸‡à¹ƒà¸«à¸¡à¹ˆà¸­à¸µà¸à¸—à¸µà¸™à¸° ðŸ™"

    def get_info(self) -> dict:
        """Return client info for debugging"""
        return {
            "provider": "groq",
            "fast_model": FAST_MODEL,
            "smart_model": SMART_MODEL,
            "has_key": bool(self.api_key),
        }
